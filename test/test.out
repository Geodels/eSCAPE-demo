--------------------------------------------------------------------------
Petsc Release Version 3.8.4, unknown 
       The PETSc Team
    petsc-maint@mcs.anl.gov
 http://www.mcs.anl.gov/petsc/
See docs/changes/index.html for recent updates.
See docs/faq.html for problems.
See docs/manualpages/index.html for help. 
Libraries linked from /live/lib/petsc/arch-linux2-c-opt/lib
--------------------------------------------------------------------------
Input file: input_test.yml
 Verbose is on? True
 PETSC log is on? True
Input file: input_test.yml
 Verbose is on? True
 PETSC log is on? True
Input file: input_test.yml
 Verbose is on? True
 PETSC log is on? True
Input file: input_test.yml
 Verbose is on? True
 PETSC log is on? True
The following model will be run:     simple model for testing installation
Reading mesh information (0.00 seconds)
Create DMPlex (0.04 seconds)
Distribute DMPlex (0.10 seconds)
Distribute field to DMPlex (0.01 seconds)
Defining Petsc DMPlex (0.17 seconds)
Voronoi creation (0.01 seconds)
Tesselation (0.01 seconds)
Finite volume mesh declaration (0.19 seconds)
Priority-flood algorithm initialisation (0.00 seconds)
Update External Forces (0.00 seconds)
--- Initialisation Phase (0.71 seconds)
Flow Direction declaration (0.00 seconds)
Compute Flow Accumulation (0.22 seconds)
Creating outputfile (0.02 seconds)
+++ Output Simulation Time: 0.00 years
Compute Stream Power Law (0.42 seconds)
Pit filling algorithm (0.00 seconds)
Pit parameters definition (0.00 seconds)
Fill Pit Depression (0.00 seconds)
Compute Sediment Diffusion (0.11 seconds)
Compute Hillslope Processes (0.00 seconds)
Update External Forces (0.00 seconds)
--- Computational Step (0.79 seconds)
Flow Direction declaration (0.00 seconds)
Compute Flow Accumulation (0.22 seconds)
Compute Stream Power Law (0.42 seconds)
Pit filling algorithm (0.00 seconds)
Pit parameters definition (0.00 seconds)
Fill Pit Depression (0.00 seconds)
Compute Sediment Diffusion (0.10 seconds)
Compute Hillslope Processes (0.00 seconds)
Update External Forces (0.00 seconds)
Creating outputfile (0.01 seconds)
+++ Output Simulation Time: 100.00 years
--- Computational Step (0.76 seconds)

-------------------------------------------------------------
                 Testing eSCAPE installation                 
-------------------------------------------------------------

Maximum deposition thickness (m):   8.4144388862
Minimum deposition thickness (m):   -1.08636116119

Maximum elevation (m):              1264.82966136
Minimum elevation (m):              19.2141465268

-------------------------------------------------------------
           Comparison with expected installation             
-------------------------------------------------------------

Difference in max deposition thickness (m):  0.00
Difference in min deposition thickness (m):  0.00

Difference in maximum elevation (m):  0.00
Difference in minimum elevation (m):  0.00


-------------------------------------------------------------
                      ending eSCAPE test                     
-------------------------------------------------------------
Cleaning Model Dataset (0.00 seconds)
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

run_escape.py on a arch-linux2-c-opt named 571c950aa1d1 with 4 processors, by Unknown Tue Sep 25 09:29:03 2018
Using Petsc Release Version 3.8.4, unknown 

                         Max       Max/Min        Avg      Total 
Time (sec):           2.931e+00      1.00003   2.931e+00
Objects:              9.640e+02      1.03879   9.380e+02
Flop:                 1.280e+07      1.51888   1.071e+07  4.285e+07
Flop/sec:            4.367e+06      1.51890   3.654e+06  1.462e+07
MPI Messages:         2.474e+03      1.55176   2.045e+03  8.180e+03
MPI Message Lengths:  6.429e+06      2.25029   1.863e+03  1.524e+07
MPI Reductions:       1.166e+03      1.00000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop -----  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total   counts   %Total     Avg         %Total   counts   %Total 
 0:      Main Stage: 2.9313e+00 100.0%  4.2849e+07 100.0%  8.180e+03 100.0%  1.863e+03      100.0%  1.165e+03  99.9% 

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                   Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   Avg. len: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                             --- Global ---  --- Stage ---   Total
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   Avg len Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s
------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSided         38 1.0 3.1166e-0212.1 0.00e+00 0.0 1.3e+02 4.0e+00 0.0e+00  1  0  2  0  0   1  0  2  0  0     0
VecMax               104 1.0 3.0339e-03 1.3 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+02  0  0  0  0  9   0  0  0  0  9     0
VecMin                 2 1.0 1.3924e-04 5.1 0.00e+00 0.0 0.0e+00 0.0e+00 2.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecNorm              156 1.0 6.2406e-03 2.1 1.15e+06 1.0 0.0e+00 0.0e+00 1.6e+02  0 10  0  0 13   0 10  0  0 13   721
VecScale               6 1.0 5.1260e-05 1.4 1.47e+04 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  1125
VecCopy              208 1.0 1.3244e-03 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecSet               256 1.0 1.2224e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecAXPY              350 1.0 2.1017e-03 1.1 2.57e+06 1.0 0.0e+00 0.0e+00 0.0e+00  0 24  0  0  0   0 24  0  0  0  4802
VecAYPX              148 1.0 7.0000e-04 1.1 5.43e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  5  0  0  0   0  5  0  0  0  3048
VecWAXPY               6 1.0 5.4836e-05 1.1 2.20e+04 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  1577
VecPointwiseMult     102 1.0 6.0844e-04 1.0 3.75e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  3  0  0  0   0  3  0  0  0  2417
VecScatterBegin      148 1.0 6.8331e-04 1.0 0.00e+00 0.0 9.0e+02 2.9e+02 0.0e+00  0  0 11  2  0   0  0 11  2  0     0
VecScatterEnd        148 1.0 1.0777e-03 1.5 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatMult              148 1.0 7.3631e-03 1.1 3.64e+06 2.3 9.0e+02 2.9e+02 0.0e+00  0 24 11  2  0   0 24 11  2  0  1424
MatSolve             156 1.0 6.4943e-03 1.4 3.85e+06 2.3 0.0e+00 0.0e+00 0.0e+00  0 26  0  0  0   0 26  0  0  0  1717
MatLUFactorNum         8 1.0 1.9944e-03 1.3 4.06e+05 1.4 0.0e+00 0.0e+00 0.0e+00  0  3  0  0  0   0  3  0  0  0   705
MatILUFactorSym        8 1.0 9.9826e-04 1.6 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatConvert             6 1.0 7.0095e-04 1.5 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatAssemblyBegin      70 1.0 7.3492e-0216.4 0.00e+00 0.0 3.6e+01 2.7e+02 1.4e+02  1  0  0  0 12   1  0  0  0 12     0
MatAssemblyEnd        70 1.0 1.4621e-02 1.1 0.00e+00 0.0 8.1e+02 5.3e+01 5.6e+02  0  0 10  0 48   0  0 10  0 48     0
MatGetRow         242352 1.0 8.3955e-01 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00 28  0  0  0  0  28  0  0  0  0     0
MatGetRowIJ            8 1.0 3.1233e-05 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatGetOrdering         8 1.0 3.1233e-04 1.5 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatAXPY               33 1.0 1.6977e+00 1.0 0.00e+00 0.0 4.7e+02 7.0e+01 4.0e+02 58  0  6  0 34  58  0  6  0 34     0
MatTranspose           2 1.0 1.9760e-03 1.0 0.00e+00 0.0 9.0e+01 1.7e+02 2.4e+01  0  0  1  0  2   0  0  1  0  2     0
PCSetUp               16 1.0 3.5203e-03 1.3 4.06e+05 1.4 0.0e+00 0.0e+00 0.0e+00  0  3  0  0  0   0  3  0  0  0   399
PCSetUpOnBlocks        8 1.0 3.6337e-03 1.3 4.06e+05 1.4 0.0e+00 0.0e+00 0.0e+00  0  3  0  0  0   0  3  0  0  0   387
PCApply              156 1.0 9.6970e-03 1.2 3.85e+06 2.3 0.0e+00 0.0e+00 0.0e+00  0 26  0  0  0   0 26  0  0  0  1150
KSPSetUp              16 1.0 1.6308e-04 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPSolve               8 1.0 2.8780e-02 1.0 1.06e+07 1.7 9.0e+02 2.9e+02 1.6e+02  1 79 11  2 13   1 79 11  2 13  1173
Mesh Partition         2 1.0 5.4066e-02 1.0 0.00e+00 0.0 3.0e+02 4.8e+03 1.1e+01  2  0  4  9  1   2  0  4  9  1     0
Mesh Migration         2 1.0 3.1560e-02 1.0 0.00e+00 0.0 4.6e+02 1.5e+04 2.0e+01  1  0  6 44  2   1  0  6 44  2     0
DMPlexInterp           1 1.0 3.5269e-02 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  1  0  0  0  0   1  0  0  0  0     0
DMPlexDistribute       1 1.0 8.6553e-02 1.0 0.00e+00 0.0 7.8e+02 1.2e+04 3.1e+01  3  0 10 61  3   3  0 10 61  3     0
DMPlexDistCones        2 1.0 8.7388e-03 1.0 0.00e+00 0.0 1.0e+02 2.6e+04 0.0e+00  0  0  1 18  0   0  0  1 18  0     0
DMPlexDistLabels       2 1.0 1.0374e-02 1.0 0.00e+00 0.0 1.8e+02 1.3e+04 2.0e+00  0  0  2 15  0   0  0  2 15  0     0
DMPlexDistribOL        1 1.0 2.1155e-02 1.0 0.00e+00 0.0 5.8e+02 9.5e+02 2.2e+01  1  0  7  4  2   1  0  7  4  2     0
DMPlexDistField        4 1.0 5.1367e-03 1.0 0.00e+00 0.0 1.7e+02 1.6e+04 4.0e+00  0  0  2 17  0   0  0  2 17  0     0
DMPlexDistData         2 1.0 2.9760e-0218.9 0.00e+00 0.0 1.8e+02 4.7e+03 0.0e+00  1  0  2  5  0   1  0  2  5  0     0
DMPlexStratify         4 1.0 4.5624e-02 2.5 0.00e+00 0.0 0.0e+00 0.0e+00 4.0e+00  1  0  0  0  0   1  0  0  0  0     0
SFSetGraph            38 1.0 3.1734e-04 1.4 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
SFBcastBegin         625 1.0 5.1414e-02 2.0 0.00e+00 0.0 6.4e+03 2.3e+03 0.0e+00  2  0 78 94  0   2  0 78 94  0     0
SFBcastEnd           625 1.0 3.5188e-02 1.4 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0
SFReduceBegin          6 1.0 4.1294e-04 1.1 0.00e+00 0.0 7.0e+01 7.6e+03 0.0e+00  0  0  1  3  0   0  0  1  3  0     0
SFReduceEnd            6 1.0 2.4033e-04 2.7 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
SFFetchOpBegin         1 1.0 1.2159e-05 2.3 0.00e+00 0.0 5.0e+00 4.1e+02 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
SFFetchOpEnd           1 1.0 7.2002e-05 3.7 0.00e+00 0.0 5.0e+00 4.1e+02 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

           Container     2              1          568     0.
              Viewer     1              0            0     0.
           Index Set   270            264      2471948     0.
   IS L to G Mapping     6              5      1073900     0.
             Section    49             41        28536     0.
              Vector   221            216      2375696     0.
      Vector Scatter    76             76        81472     0.
              Matrix   239            238     17933608     0.
      Preconditioner    16             16        15168     0.
       Krylov Solver    16             16        18816     0.
    Distributed Mesh     7              5        24600     0.
    GraphPartitioner     4              3         1812     0.
   Star Forest Graph    50             47        39464     0.
     Discrete System     7              5         4480     0.
========================================================================================================================
Average time to get PetscTime(): 3.93391e-06
Average time for MPI_Barrier(): 2.17438e-05
Average time for zero size MPI_Send(): 3.15905e-06
#PETSc Option Table entries:
-i input_test.yml
-l
-v
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --CFLAGS=-O3 --CXXFLAGS=-O3 --FFLAGS=-O3 --with-debugging=no --with-hdf5-dir=/usr/local/hdf5 --download-fblaslapack --download-ctetgen --download-metis=yes --download-parmetis=yes --download-triangle
-----------------------------------------
Libraries compiled on Wed Sep  5 04:41:24 2018 on 297267a5b828 
Machine characteristics: Linux-4.4.0-1060-aws-x86_64-with-Ubuntu-16.04-xenial
Using PETSc directory: /live/lib/petsc
Using PETSc arch: arch-linux2-c-opt
-----------------------------------------

Using C compiler: mpicc -O3 -fPIC   ${COPTFLAGS} ${CFLAGS}
Using Fortran compiler: mpif90 -O3 -fPIC   ${FOPTFLAGS} ${FFLAGS} 
-----------------------------------------

Using include paths: -I/live/lib/petsc/arch-linux2-c-opt/include -I/live/lib/petsc/include -I/live/lib/petsc/include -I/live/lib/petsc/arch-linux2-c-opt/include -I/usr/local/hdf5/include
-----------------------------------------

Using C linker: mpicc
Using Fortran linker: mpif90
Using libraries: -Wl,-rpath,/live/lib/petsc/arch-linux2-c-opt/lib -L/live/lib/petsc/arch-linux2-c-opt/lib -lpetsc -Wl,-rpath,/live/lib/petsc/arch-linux2-c-opt/lib -L/live/lib/petsc/arch-linux2-c-opt/lib -Wl,-rpath,/usr/local/hdf5/lib -L/usr/local/hdf5/lib -Wl,-rpath,/usr/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -Wl,-rpath,/usr/lib/gcc/x86_64-linux-gnu/5 -L/usr/lib/gcc/x86_64-linux-gnu/5 -Wl,-rpath,/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lflapack -lfblas -lhdf5hl_fortran -lhdf5_fortran -lhdf5_hl -lhdf5 -lparmetis -lmetis -ltriangle -lctetgen -lpthread -lm -lmpichfort -lgfortran -lm -lgfortran -lm -lquadmath -lmpichcxx -lstdc++ -lm -ldl -lmpich -lgcc_s -ldl
-----------------------------------------


+++
+++ Total run time (2.26 seconds)
+++
